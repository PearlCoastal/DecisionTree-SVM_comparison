#Here are my source code and discussion under the results compared between Decision Tree and SVM.

import pandas as pd

#I am using the dataset named "penguin".

#Using species island culumen and flipper to check whether the penguin is a male or a female

url="https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv"

df = pd.read_csv(url)

# The initial dataset contains "species", "island" and "sex" which are string and I've transformed them into float using "replace"

# First Step: Data pre-processing procedure

# Like in species there are three kinds : "Adelie, Chinstrap, Gentoo", and they are in "string"so I replace it with "0,1,2" which is in "float"

# The island and sex are pre-processed as the same

# species
df=df.replace({'Adelie' : 0, 'Chinstrap' : 1, 'Gentoo' : 2})

# island
df=df.replace({'Torgersen':0,'Dream':1,'Gentoo':2,'Biscoe':3})

# sex
df=df.replace({'MALE' : 0, 'FEMALE' : 1})


# Because there are several missing values in penguin dataset as showing like "NaN"
# so I use the dropna function in order to drop those missing indices.

df.dropna(subset= ['sex'], inplace = True)

df

# here are my pre-processed dataset

from sklearn.model_selection import train_test_split

# using 'species', 'island', 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g' as training feature
feature = df.loc[:,['species', 'island', 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]

# supervised machine learning

# 'sex' as label
target = df.loc[:,['sex']]


# seperate dataset

# training data and test data are divided as 8/10 and 2/10

# random state

x_feature, y_feature, x_target, y_target = train_test_split(feature, target, train_size=0.8, random_state=2)

x_feature

# Using Decison Tree
# construct a tree as below

from sklearn import tree

print("Train the data using Decision Tree")


# entropy is the expected value of information amount
# when entropy is high,its difficlut to seperate
# while entropy is low, its good
clf = tree.DecisionTreeClassifier(criterion='entropy')

clf=clf.fit(x_feature, x_target)
 
# Show the trained tree using plot function

print("Decision Tree is like below")

tree.plot_tree(clf) 

predicted_DT = clf.predict(y_feature)

from sklearn.metrics import confusion_matrix

# confusion matrix
confusion_matrix(y_target,predicted_DT)

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score


print("Results of Decision Tree")

print('Accuracy = ', accuracy_score(y_target, predicted_DT))

print('Precision = ', precision_score(y_target, predicted_DT))

print('Recall = ', recall_score(y_target, predicted_DT))

print('F1 score = ', f1_score(y_target, predicted_DT))

# using SVM

print("Train the data using SVM")

from sklearn import svm

clf = svm.SVC()

clf.fit(x_feature, x_target)

predicted_SVM = clf.predict(y_feature)

from sklearn.metrics import confusion_matrix

confusion_matrix(y_target,predicted_SVM)

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

print("Results of SVM")

print('Accuracy = ', accuracy_score(y_target, predicted_SVM))

print('Precision = ', precision_score(y_target, predicted_SVM))

print('Recall = ', recall_score(y_target, predicted_SVM))

print('F1 score = ', f1_score(y_target, predicted_SVM))
